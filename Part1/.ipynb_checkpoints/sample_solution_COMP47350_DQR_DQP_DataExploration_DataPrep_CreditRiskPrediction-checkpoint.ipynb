{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "This exercise focuses on data understanding and preparation for a particular problem and dataset.\n",
    "The problem and data come from a credit scoring company concerned with reducing credit repayment risk. The company wants to use the data collected about their customers to build a data analytics solution for credit risk prediction.\n",
    "The fundamental task is to use the information about the applicant in their credit report to predict whether they will repay their credit within 2 years. The target variable to predict is a binary variable called RiskPerformance. The value “Bad” indicates that a consumer was 90 days past due or worse at least once over a period of 24 months from when the credit account was opened. The value “Good” indicates that they have made their payments without ever being more than 90 days overdue. The dataset we work with is a sample of the data used in this data challenge: https://community.fico.com/s/explainable-machine-learning-challenge?tabset-3158a=2\n",
    "\n",
    "Each student will work with a different subset of the data. The CSV file is named using the format: **CreditRisk-[your-student-number].csv**, e.g., **CreditRisk-12345678.csv** is the data file for a student with number 12345678. You need to work with the CSV file corresponding to your student number. There are 4 parts for this homework. Each part has an indicative maximum percentage given in brackets, e.g., part (1) has a maximum of 40% shown as [40].\n",
    "\n",
    "\n",
    "\n",
    "(1). [40] Prepare a data quality report for your CSV file. Below you have a set of guideline steps to help you in this process.\n",
    "    - Check how many rows and columns your CSV has.\n",
    "    - Print the first and the last 5 rows.\n",
    "    - Convert the features to their appropriate data types (e.g., decide which features are more appropriate as \n",
    "    continuos and which ones as categorical types). \n",
    "    - Drop duplicate rows and columns, if any.\n",
    "    - Drop constant columns, if any.\n",
    "    - Save your updated/cleaned data frame to a new csv file.\n",
    "  \n",
    "    For the updated CSV and data frame (after column/row removal):\n",
    "    - Prepare a table with descriptive statistics for all the continuous features.\n",
    "    - Prepare a table with descriptive statistics for all the categorical features.\n",
    "    - Plot histograms for all the continuous features.\n",
    "    - Plot box plots for all the continuous features.\n",
    "    - Plot bar plots for all the categorical features.\n",
    "    - Discuss your initial findings.\n",
    "    - Save the initial discussion of your findings into a single data quality report PDF file.                    The PDF report should focus on the key issues identified in the data and discuss potential strategies              to handle them. Simple listing of tables and plots without discussion and justification will not receive full marks. \n",
    "\n",
    "(2). [30] Prepare a data quality plan for the cleaned CSV file. \n",
    "    - Mark down all the features where there are potential problems or data quality issues.\n",
    "    - Propose solutions to deal with the problems identified. Explain why did you choose one solution over \n",
    "    potentially many other.\n",
    "    - Apply your solutions to obtain a new CSV file where the identified data quality issues were addressed. \n",
    "    - Save the new CSV file with a self explanatory name. \n",
    "    - Save the data quality plan to a single PDF file.\n",
    "        \n",
    "(3). [15] Exploring relationships between feature pairs:\n",
    "    - Choose a subset of features you find promising and plot pairwise feature interactions (e.g., \n",
    "    continuous-continuous feature plot or continuous-categorical plots or correlation plots). \n",
    "    Explain your choices.\n",
    "    - Discuss your findings from the plots above. Do you find any features or feature combinations that are \n",
    "    indicative of the target outcome? Explain in plain words (a short paragraph) the story of your\n",
    "    findings so far.\n",
    "    \n",
    "(4). [15] Transform, extend or combine the existing features to create a few new features (at least 3) with the aim to better capture the problem domain and the target outcome. Justify the steps and choices you are making. Add these features to your clean dataset and save it as a CSV file with a self explanatory name. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (0). Background for the given data analytics problem and dataset.\n",
    "Please check the PDF report \"Data_Quality_Report_and_Plan_Initial_Findings.pdf\" for background and important terminology for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas, numpy, matplotlib, seaborn libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# hide ipykernel warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Specified a sep and a delimiter; you can only specify one.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21557/1163992729.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# read in data from csv file to pandas dataframe.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CreditRisk.csv'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mkeep_default_na\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m',\\s+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipinitialspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    663\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sep\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m     kwds_defaults = _refine_defaults_read(\n\u001b[0m\u001b[1;32m    666\u001b[0m         \u001b[0mdialect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0mdelimiter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_refine_defaults_read\u001b[0;34m(dialect, delimiter, delim_whitespace, engine, sep, error_bad_lines, warn_bad_lines, on_bad_lines, names, prefix, defaults)\u001b[0m\n\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdelimiter\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_default\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1509\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Specified a sep and a delimiter; you can only specify one.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1511\u001b[0m     if (\n",
      "\u001b[0;31mValueError\u001b[0m: Specified a sep and a delimiter; you can only specify one."
     ]
    }
   ],
   "source": [
    "# read in data from csv file to pandas dataframe.  \n",
    "df = pd.read_csv('CreditRisk.csv',  keep_default_na=True, sep=',\\s+', delimiter=',', skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1). Prepare a data quality report for the CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check shape of CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has 1000 entries with 24 features per entry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print first and last 5 rows of CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert features to appropriate datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on examining the data in a spreadsheet program, 3 of the features are categorical and the rest continuous.\n",
    "\n",
    "- *RiskPerformance* will be converted to **categorical** because it contains only 2 possible values, true of false.\n",
    "- *'MaxDelq2PublicRecLast12M'* column will be converted to **categorical** because it contains a finite set of possible values, *(0-9)*, each representing a different meaning, thus a different category.\n",
    "- *'MaxDelqEver'* column will be converted to **categorical** because it contains a finite set of possible values, *(1-9)*, each representing a different meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns containing categorical data\n",
    "categorical_columns = df[['MaxDelq2PublicRecLast12M','MaxDelqEver','RiskPerformance']].columns\n",
    "# MaxDelq2PublicRecLast12M and MaxDelqEver were chosen as categories because \n",
    "# the data contains single digit values each representing a different meaning\n",
    "\n",
    "# Convert data type to category for these columns\n",
    "for column in categorical_columns:\n",
    "    df[column] = df[column].astype('category')  \n",
    "\n",
    "continuous_features = df.select_dtypes(['int64']).columns\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converted *RiskPerformance*, *'MaxDelq2PublicRecLast12M'* & *'MaxDelqEver'* to categorical type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for duplicate row/columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the number of duplicates, without the original rows that were duplicated\n",
    "print('Number of duplicate (excluding first) rows in the table is: ', df.duplicated().sum())\n",
    "\n",
    "# Check for duplicate rows. \n",
    "# Use \"keep=False\" to mark all duplicates as true, including the original rows that were duplicated.\n",
    "print('Number of duplicate rows (including first) in the table is:', df[df.duplicated(keep=False)].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate columns\n",
    "#First transpose the df so columns become rows, then apply the same check as above\n",
    "dfT = df.T\n",
    "print(\"Number of duplicate (excluding first) columns in the table is: \", dfT.duplicated().sum())\n",
    "print(\"Number of duplicate (including first) columns in the table is: \",  dfT[dfT.duplicated(keep=False)].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result - Duplicate rows exist\n",
    "\n",
    "- Rows - There is no primary key in the dataset (id or customer number attribute) to check if a user is included twice. Therefore I compare against all columns attributes. This acts like a primary key. This initially returned 54 duplicate rows. To also include the rows that were matched I include \"keep=False\" parameter. Displaying these rows in a dataframe I see they include the value -9 for all attributes. This is a special value meaning \"No Bureau Record or No Investigation\" information is available. These rows can be safely dropped as they add no information.\n",
    "- Columns - There are no duplicate columns. We will also check the descriptive stats to make sure we did not miss any duplicated columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplicate rows can be seen below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show duplicate row data that can be dropped\n",
    "df[df.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows and columns in this toy df that atually has duplicated rows and columns, \n",
    "#to check the duplicate checks work correctly\n",
    "toy_duplicates_df = df[df.duplicated(keep=False)].iloc[:,0:5].head()\n",
    "toy_duplicates_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of duplicate (excluding first) rows in the toy table is: ', toy_duplicates_df.duplicated().sum())\n",
    "print('Number of duplicate rows (including first) in the toy table is:', toy_duplicates_df[toy_duplicates_df.duplicated(keep=False)].shape[0])\n",
    "\n",
    "#transpose the toy df\n",
    "toy_duplicates_dfT = toy_duplicates_df.T\n",
    "print(\"\\nNumber of duplicate (excluding first) columns in the toy table is: \", toy_duplicates_dfT.duplicated().sum())\n",
    "print(\"Number of duplicate (including first) columns in the toy table is: \",  toy_duplicates_dfT[toy_duplicates_dfT.duplicated(keep=False)].shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The duplicate checks seems to be working as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duplicate rows will now be dropped from the original data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(keep=False)\n",
    "# get duplicated row data. Use \"keep=False\" to mark all duplicates as true\n",
    "print('Duplicate remaining rows:', df[df.duplicated(keep=False)].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for constant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print table with categorical statistics\n",
    "df.select_dtypes(['category']).describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorical Data** - Reviewing the categorical data below we can see all unique values > 1\n",
    "- *RiskPerformance* has 2 unique values\n",
    "- *MaxDelq2PublicRecLast12M* has 8 unique values\n",
    "- *MaxDelqEver* has 7 unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print table with continuous statistics\n",
    "df.select_dtypes(include=['int64']).describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Continuous Data** - Reviewing the continuous data below we can see all have a non zero standard deviation.  \n",
    "- This implies that a particular feature does not contain a single constant value in all of the rows. Thus in this case, none of the continuous features are constant.\n",
    "- Result - No constant columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Result - No null values directly in the spreadsheet, but we will check carefully if Null values are not coded differently in this dataset. It could be that data is missing but it was not coded as Null in the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Irregular cardinalities & permitted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for irregular cardinality & permitted values in categorical features. \n",
    "print(\"Unique values for:\\nMax deliquency in last 12 months:\", pd.unique(df[\"MaxDelq2PublicRecLast12M\"].ravel()))\n",
    "print(\"Max delinquency ever:\", pd.unique(df[\"MaxDelqEver\"].ravel()))\n",
    "print(\"Risk performance:\", pd.unique(df[\"RiskPerformance\"].ravel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no irregular cardinalities. The values that appear seem to be in the permitted range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics: Continuous Features\n",
    "Of the continuous features, we already know:\n",
    "-  The type of data for each\n",
    "\n",
    "<br>\n",
    "We now supplement this information with the following\n",
    "\n",
    "-  The range into which each feature can fall\n",
    "-  A numerical representation of how these values are distributed across the upper tiers of that range \n",
    "-  Feature cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each continuous feature, we display the range of values it takes.\n",
    "# We also display the number of instances each of its values has.\n",
    "\n",
    "for feature in continuous_features:\n",
    "    print(feature)\n",
    "    print(\"----------\\n\")\n",
    "    print(\"Range {} is: \".format(feature), (df[feature].max() - df[feature].min()))\n",
    "    print(\"----------\")\n",
    "    print('{0:.5}  {1}'.format(\"Value\", \"Number of Instances\"))\n",
    "    print(df[feature].value_counts().nlargest(15), \"\\n\\n\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the cardinality of each continuous feature\n",
    "\n",
    "features_cardinality = list(df[continuous_features].columns.values)\n",
    "\n",
    "print('{0:35}  {1}'.format(\"Feature\", \"Cardinality\"))\n",
    "print('{0:35}  {1}'.format(\"-------\", \"--------------- \\n\"))\n",
    "\n",
    "for c in features_cardinality:\n",
    "    print('{0:35}  {1}'.format(c, str(len(df[c].unique()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running Assessment:**\n",
    "<br><br>\n",
    "\n",
    "-  Examining the gaps / relationships between the minimum and maximum values and the quartile ranges, the features listed below may contain significant outliers. This is not a definitive list. More insight will be gained from visual analysis of these data presented on histograms and boxplots, below.\n",
    "<br>\n",
    "\n",
    " -  NumTrades60Ever2DerogPubRec\n",
    " -  NumTrades90Ever2DerogPubRec\n",
    " -  MSinceMostRecentDelq\n",
    " -  NumTotalTrades\n",
    " -  MSinceMostRecentInqexcl7days \n",
    "<br><br>\n",
    "\n",
    "-  It is clear from both visual analysis of the rows and from this initial descriptive analysis that -7 and -8 are also special values in the original data.\n",
    "<br><br>\n",
    "\n",
    " -  Examining the gaps / relationships between the minimum first quartile ranges, it is evident that these special values are left-skewing the data.\n",
    " \n",
    " **Action:**\n",
    "<br><br>\n",
    "\n",
    "-  We shall count the instances of each special feature for each value, and express the count as a percentage of the total values for that feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each continuous feature, print the number of occurrences of the remaining special values, \n",
    "# and the percentage of the features total values that they represent.\n",
    "\n",
    "print(\"SPECIAL VALUE COUNT\\n---------------\")\n",
    "\n",
    "for col in continuous_features:\n",
    "    print(\"Feature:\", col, \"\\t\")\n",
    "    print(len(df[df[col] == -8.0]), \"occurrences of -8 special value ( = \", round((((len(df[df[col] == -8.0]))/ len(df[col]))*100), 2), \"% of values)\")\n",
    "    print(len(df[df[col] == -7.0]), \"occurrences of -7 special value ( = \", round((((len(df[df[col] == -7.0]))/ len(df[col]))*100), 2), \"% of values)\")\n",
    "    print(\"\\n\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running Assessment:**\n",
    "\n",
    "-  We shall take note of this data quality issue and address it in the Data Quality Plan.\n",
    "-  We shall continue to visually represent the data as it stands.\n",
    "-  We expect some of the results to be skewed because of the negative, special values. \n",
    "We shall retain this original representation of the data for further comparison and documentation.\n",
    "\n",
    "The meanings of the additional special values are as follows:\n",
    "<br><br>\n",
    "\n",
    "-  \"-8 No Usable/Valid Accounts Trades or Inquiries\"\n",
    "-  \"-7 Condition not Met (e.g. No Inquiries, No Delinquencies) \"\n",
    " -  Source: https://community.fico.com/s/explainable-machine-learning-challenge?tabset-3158a=413df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check logical integrity of data \n",
    "A number of additional checks will be performed to see if the data makes sense. Depending on the amount of data affected we may decide to drop those rows or replace the values upon consultation with a domain expert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Test 1 Check if any entries have number of satisfactory trades > than number of total trades (impossible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1 = df[['NumSatisfactoryTrades','NumTotalTrades']][df['NumSatisfactoryTrades']>df['NumTotalTrades']]\n",
    "print(\"Number of rows failing the test: \", test_1.shape[0])\n",
    "test_1.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Test 2 Check if any entries have number of enquiries in last 6 months excluding last 7 days > number of enquiries in last 6 months (impossible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2 = df[['NumInqLast6M','NumInqLast6Mexcl7days']][df['NumInqLast6Mexcl7days']>df['NumInqLast6M']]\n",
    "print(\"Number of rows failing the test: \", test_2.shape[0])\n",
    "test_2.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test 3 Check if any entries have number trades open in last 12 months > number of total trades (impossible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_3 = df[['NumTradesOpeninLast12M','NumTotalTrades']][df['NumTradesOpeninLast12M']>df['NumTotalTrades']]\n",
    "print(\"Number of rows failing the test: \", test_3.shape[0])\n",
    "test_3.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Test 4 Check if any entries have number trades 90 days late > number of total trades 60 days late (impossible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_4 = df[['NumTrades90Ever2DerogPubRec','NumTrades60Ever2DerogPubRec']][df['NumTrades90Ever2DerogPubRec']>df['NumTrades60Ever2DerogPubRec']]\n",
    "print(\"Number of rows failing the test: \", test_4.shape[0])\n",
    "test_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test 5 Check if any entries have number trades 90 days late > number of total trades (impossible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_5 = df[['NumTrades90Ever2DerogPubRec','NumTotalTrades']][df['NumTrades90Ever2DerogPubRec']>df['NumTotalTrades']]\n",
    "print(\"Number of rows failing the test: \", test_5.shape[0])\n",
    "test_5.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Test 6 Check if any entries have value for number months since most recent trade > number months since oldest trade (impossible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_6 = df[['MSinceMostRecentTradeOpen','MSinceOldestTradeOpen']][df['MSinceMostRecentTradeOpen']>df['MSinceOldestTradeOpen']]\n",
    "print(\"Number of rows failing the test: \", test_6.shape[0])\n",
    "test_6.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Many of these failures are due to -8 special value which will be handled in the next section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test 7 Check if any entries have number revolving trades with balance > number total trades (impossible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_7 = df[['NumRevolvingTradesWBalance','NumTotalTrades']][df['NumRevolvingTradesWBalance']>df['NumTotalTrades']]\n",
    "print(\"Number of rows failing the test: \", test_7.shape[0])\n",
    "test_7.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Test 8 Check if any entries have number install trades with balance > number total trades (impossible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_8 = df[['NumInstallTradesWBalance','NumTotalTrades']][df['NumInstallTradesWBalance']>df['NumTotalTrades']]\n",
    "print(\"Number of rows failing the test: \", test_8.shape[0])\n",
    "test_8.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test 9 check if any entries have number bank trades with high utilization > number total trades (impossible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_9 = df[['NumBank2NatlTradesWHighUtilization','NumTotalTrades']][df['NumBank2NatlTradesWHighUtilization']>df['NumTotalTrades']]\n",
    "print(\"Number of rows failing the test: \", test_9.shape[0])\n",
    "test_9.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Test 10 Check for entries that have no trades but entry for months since trade open (impossible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_10 = df[['NumTotalTrades','MSinceMostRecentTradeOpen']][df['NumTotalTrades']==0][df[\"MSinceMostRecentTradeOpen\"]>=0]\n",
    "print(\"Number of rows failing the test: \", test_10.shape[0])\n",
    "test_10.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -  Test 11 Check for % trades never delinquent == 100% and have a trade over 60 days late (impossible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_11 = df[['PercentTradesNeverDelq','NumTrades60Ever2DerogPubRec']][df['PercentTradesNeverDelq']==100][df[\"NumTrades60Ever2DerogPubRec\"]>0]\n",
    "print(\"Number of rows failing the test: \", test_11.shape[0])\n",
    "test_11.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -  Test 12 Check for % trades never delinquent < 100% and have no trade over 60 days late (impossible)\n",
    "      - Check with domain expert - we do not have any feature for number of trades 30 days late which could account for this. No action will be taken as there are too many rows effected "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_12 = df[['PercentTradesNeverDelq','NumTrades60Ever2DerogPubRec']][df['PercentTradesNeverDelq']<100][df[\"NumTrades60Ever2DerogPubRec\"]==0]\n",
    "print(\"Number of rows failing the test: \", test_12.shape[0])\n",
    "test_12.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -  Test 13 Check for months since most recent delinquency == 0 (i.e. just delinquent) and have no trade over 60 days late (impossible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_13 = df[['MSinceMostRecentDelq','NumTrades60Ever2DerogPubRec']][df['MSinceMostRecentDelq']==0][df[\"NumTrades60Ever2DerogPubRec\"]==0]\n",
    "print(\"Number of rows failing the test: \", test_13.shape[0])\n",
    "test_13.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Save updated/cleaned data frame to a new csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the dataframe to a csv file\n",
    "df.to_csv('CreditRisk_1-1_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Prepare a table with descriptive statistics for all the continuous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print table with descriptive statistics for all the continuous features\n",
    "continuous_columns = df.select_dtypes(['int64']).columns\n",
    "df[continuous_columns].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Save descriptive statistics for all the continuous features to csv for data quality report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuousFeatueDesc = df[continuous_columns].describe().T\n",
    "continuousFeatueDesc.to_csv(\"continuousFeatureDescription.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Prepare a table with descriptive statistics for all the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print table with descriptive statistics for all the categorical features\n",
    "#df.select_dtypes(['category']).describe().T\n",
    "categorical_columns = df.select_dtypes(['category']).columns\n",
    "df[categorical_columns].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Save descriptive statistics for all the categorical features to csv for data quality report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricalFeatueDesc = df[categorical_columns].describe().T\n",
    "categoricalFeatueDesc.to_csv(\"categoricalFeatureDescription.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Plot histograms summary sheet for all the continuous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram summary sheet of the continuous features and save in a png file\n",
    "df[continuous_columns].hist(layout=(6, 4), figsize=(30,30), bins=10)\n",
    "plt.savefig('continuous_histograms_1-1.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Plot histograms individual sheet for all the continuous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in continuous_columns:\n",
    "        f = df[col].plot(kind='hist', figsize=(10,5), bins=20)\n",
    "        plt.title(col)\n",
    "        plt.ylabel('number of entries')\n",
    "        plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Plot box plots summary sheet for all the continuous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[continuous_columns].plot(kind='box', subplots=True, figsize=(30,30), layout=(6,4), sharex=False, sharey=False)\n",
    "plt.savefig('continuous_boxplots__summary1-1.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Plot box plots individual sheet for all the continuous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot box plots for all the continuous features and save in a PDF file\n",
    "with PdfPages('continuous_boxplots_1-1.pdf') as pp:\n",
    "    for col in continuous_columns:\n",
    "        f = df[col].plot(kind='box', figsize=(10,5))\n",
    "        pp.savefig(f.get_figure())\n",
    "        plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Plot bar plots individual sheet for all the categorical features.\n",
    "\n",
    "Only 3 features - No summary sheet needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bar plots for all the catagorical features and save them in a single PDF file\n",
    "with PdfPages('categorical_barplots_1-1.pdf') as pp:\n",
    "    for col in categorical_columns:\n",
    "        f = df[col].value_counts().plot(kind='bar', figsize=(12,10))\n",
    "        plt.title(col)\n",
    "        plt.ylabel('number of people')\n",
    "        pp.savefig(f.get_figure())\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for irregular cardinality & permitted values in categorical features. \n",
    "print(\"Unique values for:\\n- Max dequency in last 12 months:\", pd.unique(df[\"MaxDelq2PublicRecLast12M\"].ravel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not clear why -9 value is in the MaxDelqEver plot. As we can see from the cardinality check there is no -9 value in dataframe. Probably it is kept in the index of the dataframe, even after dropping.\n",
    "\n",
    "**Running Assessment:**\n",
    "\n",
    "-  The **cardinalities** of the Categorical features make sense. The low value is explained as being a binary, as previously noted; and there are no very high values which could either be anomolous or present problems for machine learning algorithms. \n",
    "<br>\n",
    "\n",
    "-  There are no **missing values** and no cases of this dataset's remaining special values in the Categorical features.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Findings\n",
    "\n",
    "### Initial findings and detailed discussion for each feature can be found in the accompanying *Data_Quality_Report_Initial_Findings.pdf* file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2). Data Understanding: Data Quality Plan for the cleaned CSV file.\n",
    "\n",
    "### The initial list of issues as identified in the **Data_Quality_Report_Initial_Findings.pdf**:\n",
    "\n",
    "- Special value -9 appeared in 56 rows covering all features. These rows were dropped as part of the initial cleaning as they contained no usable information.\n",
    "- Special value -8 appears in 9 features. It indicates no usable values found.\n",
    "- Special value -7 appears in 2 features. *MSinceMostRecentInqexcl7days* & *MSinceMostRecentDelq*. It indicates condition not met.\n",
    "- Different scales used for similar features - Categorical features *MaxDelqE2PublicREcLast12M* & *MaxDelqEver* are measuring the same event but are using different numerical mappings. One mapping needs to be picked for consistency.\n",
    "- Presence of outliers - There are a significant number of outliers present across a range of different features. They initially look plausible but will need to be investigated further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Propose solutions to deal with the problems identified. \n",
    "\n",
    "1. **Special value -7**\n",
    "    -  Appears in 2 features. *MSinceMostRecentInqexcl7days* & *MSinceMostRecentDelq*. The -7 value indicates “Condition not Met”. This is a positive marker for the features below representing the best possible case and therefore will need to be set as the largest value in the features that they appear\n",
    "- **Special value -8** \n",
    "    -  Appears in 9 features. Imputation will be performed where possible. Otherwise values will be changed to \"null\", with a note to revisit those features later.\n",
    "- **Remap scales used for similar features**\n",
    "    -  Categorical features *MaxDelqE2PublicREcLast12M* & *MaxDelqEver* are measuring the same event (Delinquency) but are using different numerical mappings. One mapping needs to be picked for consistency.\n",
    "    -  Re-map “unknown delinquency” value to “current or never delinquent”  \n",
    "- **Presence of outliers**\n",
    "    -  There are a significant number of outliers present across a range of different features. They initailly look plausible but will need to be investigated further. If they don't make sense they will be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Apply your solutions to obtain a new CSV file where the identified data quality issues were addressed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Number of entries with Special value -7**\n",
    "  -  Appears in 2 features. *MSinceMostRecentInqexcl7days* & *MSinceMostRecentDelq*. The -7 value indicates “Condition not Met”. This is a positive marker (monotonically decreasing) for the features below representing the best possible case and therefore will need to be set as the largest value in the features that they appear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Special value -7 \\t\\tcount\")\n",
    "print(\"MSinceMostRecentDelq\\t\\t\", df[df['MSinceMostRecentDelq']== -7 ].shape[0])\n",
    "print(\"MSinceMostRecentInqexcl7days\\t\", df[df['MSinceMostRecentInqexcl7days']== -7].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - MSinceMostRecentDelq: The largest value for most recent delinquency is 83 months. We will set -7 values to be 1.5 * (largest value) = 125months\n",
    "   - MSinceMostRecentInqexcl7days: The largest value for most recent delinquency is 24 months. We will set -7 values to be 1.5 * (largest value) = 36months\n",
    "   - The reason for picking different values for each feature is to keep the values highest values proportionate. Ideally we would speak to the domain expert to get a idea of the most appropriate value to set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Max value for feature MSinceMostRecentDelq is :\", df['MSinceMostRecentDelq'].max())\n",
    "print(\"The value that will replace -7 for feature MSinceMostRecentDelq is 1.5 * (largest value):\", np.ceil(1.5 * df['MSinceMostRecentDelq'].max()))\n",
    "\n",
    "df['MSinceMostRecentDelq'] = df['MSinceMostRecentDelq'].replace(-7, np.ceil(1.5 * df['MSinceMostRecentDelq'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Max value for feature MSinceMostRecentDelq is :\", df['MSinceMostRecentInqexcl7days'].max())\n",
    "print(\"The value that will replace -7 for feature MSinceMostRecentDelq is 1.5 * (largest value):\", np.ceil(1.5 * df['MSinceMostRecentInqexcl7days'].max()))\n",
    "\n",
    "df['MSinceMostRecentInqexcl7days'] = df['MSinceMostRecentInqexcl7days'].replace(-7, np.ceil(1.5 * df['MSinceMostRecentInqexcl7days'].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Number of entries with Special value -8**\n",
    "    -  According to the documentation provided by FICO, a value of -8 indicates “No Usable/Valid Trades or Inquiries”.\n",
    "    -  We want to avoid dropping any features if at all possible \n",
    "    -  Below a decision is made for each feature that contains -8 values, weather imputation is feasible or if the values will be changed to null.\n",
    "    - To aid in the decision the coefficient of variation will be checked. If < 1 imputation with the median will be performed. If > 1 the data will be evaluated to see if imputation with the mean is practical.\n",
    "    -  For cases where imputation with the mean/median is not possible due to high standard deviation, it would be possible to perform regression analysis to predict what the value would have been based on a group of the nearest neighbours. This however is deemed beyond the scope of this assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2.1 MSinceOldestTradeOpen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Special value -8 \\tcount\")\n",
    "print(\"MSinceOldestTradeOpen\\t\", df[df['MSinceOldestTradeOpen']== -8].shape[0])\n",
    "print()\n",
    "coeff_var = df['MSinceOldestTradeOpen'].std()/df['MSinceOldestTradeOpen'].mean()\n",
    "print(\"Coefficient of variation is: \", round(coeff_var,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   -  MSinceOldestTradeOpen: We can perform imputation on these values as there are very few rows affected and the standard deviation is relatively low. Coefficient of variation is < 1. We will replace with the median value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The median value that will be used is: \", df['MSinceOldestTradeOpen'].median())\n",
    "df['MSinceOldestTradeOpen'] = df['MSinceOldestTradeOpen'].replace(-8, df['MSinceOldestTradeOpen'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2.2 MSinceMostRecentDelq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Special value -8 \\tcount\")\n",
    "print(\"MSinceMostRecentDelq\\t\", df[df['MSinceMostRecentDelq']== -8 ].shape[0])\n",
    "print()\n",
    "coeff_var = df['MSinceMostRecentDelq'].std()/df['MSinceMostRecentDelq'].mean()\n",
    "print(\"Coefficient of variation is: \", round(coeff_var,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSinceMostRecentDelq: We can perform imputation on these values as the standard deviation is relatively low. Coefficient of variation is < 1. We will replace with the median value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The median value that will be used is: \", df['MSinceMostRecentDelq'].median())\n",
    "df['MSinceMostRecentDelq'] = df['MSinceMostRecentDelq'].replace(-8, df['MSinceMostRecentDelq'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2.3 MSinceMostRecentInqexcl7days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Special value -8 \\t\\tcount\")\n",
    "print(\"MSinceMostRecentInqexcl7days\\t\", df[df['MSinceMostRecentInqexcl7days']== -8].shape[0])\n",
    "print()\n",
    "coeff_var = df['MSinceMostRecentInqexcl7days'].std()/df['MSinceMostRecentInqexcl7days'].mean()\n",
    "print(\"Coefficient of variation is: \", round(coeff_var,2))\n",
    "print(\"Number of entries with value = 0:\", df['MSinceMostRecentInqexcl7days'][df['MSinceMostRecentInqexcl7days']<=1].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSinceMostRecentInqexcl7days: The standard deviation is high, with a coefficient of variation of 1.78. This is because 171 entries had a value of -7. See section dealing with special value -7 above. These were replaced by a value of 36 months. Ideally a regression analysis would be performed here to determine the most appropriate value but is outside the scope of the assignment. In light of this and considering the relatively small number of rows affected, replacing with the median value is the most appropriate since over 55%  of all entries are the median value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The median value that will be used is: \", df['MSinceMostRecentInqexcl7days'].median())\n",
    "df['MSinceMostRecentInqexcl7days'] = df['MSinceMostRecentInqexcl7days'].replace(-8, df['MSinceMostRecentInqexcl7days'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2.4 NetFractionRevolvingBurden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Special value -8 \\t\\tcount\")\n",
    "print(\"NetFractionRevolvingBurden\\t\", df[df['NetFractionRevolvingBurden']== -8].shape[0])\n",
    "print()\n",
    "coeff_var = df['NetFractionRevolvingBurden'].std()/df['NetFractionRevolvingBurden'].mean()\n",
    "print(\"Coefficient of variation is: \", round(coeff_var,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NetFractionRevolvingBurden: We can perform imputation on these values as the standard deviation is relatively low. Coefficient of variation is < 1. We will replace with the median value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The median value that will be used is: \", df['NetFractionRevolvingBurden'].median())\n",
    "df['NetFractionRevolvingBurden'] = df['NetFractionRevolvingBurden'].replace(-8, df['NetFractionRevolvingBurden'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2.5 NetFractionInstallBurden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Special value -8 \\tcount\")\n",
    "print(\"NetFractionInstallBurden\", df[df['NetFractionInstallBurden']== -8].shape[0])\n",
    "print()\n",
    "coeff_var = df['NetFractionInstallBurden'].std()/df['NetFractionInstallBurden'].mean()\n",
    "print(\"Coefficient of variation is: \", round(coeff_var,2))\n",
    "\n",
    "print(\"\\nMedian value is: \", df['NetFractionInstallBurden'].median(), \"\\n\")\n",
    "print(\"Mean value is: \", df['NetFractionInstallBurden'].mean(), \"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NetFractionInstallBurden: The coefficient of variation (COV) is just under 1. However looking at the plot the spread of the data too wide. Seeing as there is over 300 entries to change it does not seem practical to change all 300 rows to the median. This will skew the results too much. It seems more appropriate to change the values to null, in this case. This is not ideal as it still leaves Nan in the dataset, so we may come back to this decision later, to decide whether to drop this feature or still do imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NetFractionInstallBurden'] = df['NetFractionInstallBurden'].replace(-8, np.nan)\n",
    "print(\"Values changed to null: \", df['NetFractionInstallBurden'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2.6 NumRevolvingTradesWBalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Special value -8 \\tcount\")\n",
    "print(\"NumRevolvingTradesWBalance\", df[df['NumRevolvingTradesWBalance']== -8].shape[0])\n",
    "print()\n",
    "coeff_var = df['NumRevolvingTradesWBalance'].std()/df['NumRevolvingTradesWBalance'].mean()\n",
    "print(\"Coefficient of variation is: \", round(coeff_var,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumRevolvingTradesWBalance: The standard deviation is quite low. Imputation with the median will be performed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The median value that will be used is: \", df['NumRevolvingTradesWBalance'].median())\n",
    "df['NumRevolvingTradesWBalance'] = df['NumRevolvingTradesWBalance'].replace(-8, df['NumRevolvingTradesWBalance'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2.7 NumInstallTradesWBalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Special value -8 \\tcount\")\n",
    "print(\"NumInstallTradesWBalance\", df[df['NumInstallTradesWBalance']== -8].shape[0])\n",
    "print()\n",
    "coeff_var = df['NumInstallTradesWBalance'].std()/df['NumInstallTradesWBalance'].mean()\n",
    "print(\"Coefficient of variation is: \", round(coeff_var,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumInstallTradesWBalance: The coefficient of variation (COV) is relatively high at 2.02. However the mean is low and this appears to be skewing the COV to a high value. Looking at the histogram it seems appropriate to change -8 values to the median. Imputation with the median will be performed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The median value that will be used is: \", df['NumInstallTradesWBalance'].median())\n",
    "df['NumInstallTradesWBalance'] = df['NumInstallTradesWBalance'].replace(-8, df['NumInstallTradesWBalance'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2.8 NumBank2NatlTradesWHighUtilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Special value -8 \\tcount\")\n",
    "print(\"NumBank2NatlTradesWHighUtilization\", df[df['NumBank2NatlTradesWHighUtilization']== -8].shape[0])\n",
    "print()\n",
    "coeff_var = df['NumBank2NatlTradesWHighUtilization'].std()/df['NumBank2NatlTradesWHighUtilization'].mean()\n",
    "print(\"Coefficient of variation is: \", round(coeff_var,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumBank2NatlTradesWHighUtilization: The coefficient of variation (COV) is very high at 4.57. However the mean is very low and this appears to be skewing the COV to a high value. Looking at the histogram it seems appropriate to change -8 values to the median. Imputation with the median will be performed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The median value that will be used is: \", df['NumBank2NatlTradesWHighUtilization'].median())\n",
    "df['NumBank2NatlTradesWHighUtilization'] = df['NumBank2NatlTradesWHighUtilization'].replace(-8, df['NumBank2NatlTradesWHighUtilization'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2.9 PercentTradesWBalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Special value -8 \\tcount\")\n",
    "print(\"PercentTradesWBalance\\t\", df[df['PercentTradesWBalance']== -8].shape[0])\n",
    "print()\n",
    "coeff_var = df['PercentTradesWBalance'].std()/df['PercentTradesWBalance'].mean()\n",
    "print(\"Coefficient of variation is: \", round(coeff_var,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PercentTradesWBalance: The coefficient of variation is low and there is only one -8 value. Imputation with the median will be performed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The median value that will be used is: \", df['PercentTradesWBalance'].median())\n",
    "df['PercentTradesWBalance'] = df['PercentTradesWBalance'].replace(-8, df['PercentTradesWBalance'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Drop rows failing logical test from part 1\n",
    " -  Tests 1,3,5,6,7,8,9,10 had failures and will now be handled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of rows dropped: \", test_1.shape[0])\n",
    "df = df.drop(test_1.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now retest the remaining failures again as it is likely that many of the inconsistencies have been taken care of by removing the 74 rows above. Cells with errors are likely to be grouped together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_3 = df[['NumTradesOpeninLast12M','NumTotalTrades']][df['NumTradesOpeninLast12M']>df['NumTotalTrades']].index\n",
    "print(\"Total number of rows still to be dropped: \", test_3.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_5 = df[['NumTrades90Ever2DerogPubRec','NumTotalTrades']][df['NumTrades90Ever2DerogPubRec']>df['NumTotalTrades']].index\n",
    "print(\"Total number of rows still to be dropped: \", test_5.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_6 = df[['MSinceMostRecentTradeOpen','MSinceOldestTradeOpen']][df['MSinceMostRecentTradeOpen']>df['MSinceOldestTradeOpen']].index\n",
    "print(\"Total number of rows still to be dropped: \", test_6.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_7 = df[['NumRevolvingTradesWBalance','NumTotalTrades']][df['NumRevolvingTradesWBalance']>df['NumTotalTrades']].index\n",
    "print(\"Total number of rows still to be dropped: \", test_7.shape[0])\n",
    "df = df.drop(test_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_8 = df[['NumInstallTradesWBalance','NumTotalTrades']][df['NumInstallTradesWBalance']>df['NumTotalTrades']].index\n",
    "print(\"Total number of rows still to be dropped: \", test_8.shape[0])\n",
    "df = df.drop(test_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_9 = df[['NumBank2NatlTradesWHighUtilization','NumTotalTrades']][df['NumBank2NatlTradesWHighUtilization']>df['NumTotalTrades']].index\n",
    "print(\"Total number of rows still to be dropped: \", test_9.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_10 = df[['NumTotalTrades','MSinceMostRecentTradeOpen']][df['NumTotalTrades']==0][df[\"MSinceMostRecentTradeOpen\"]>=0].index\n",
    "print(\"Total number of rows still to be dropped: \", test_10.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_11 = df[['PercentTradesNeverDelq','NumTrades60Ever2DerogPubRec']][df['PercentTradesNeverDelq']==100][df[\"NumTrades60Ever2DerogPubRec\"]>0].index\n",
    "print(\"Total number of rows still to be dropped: \", test_11.shape[0])\n",
    "df = df.drop(test_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_13 = df[['MSinceMostRecentDelq','NumTrades60Ever2DerogPubRec']][df['MSinceMostRecentDelq']==0][df[\"NumTrades60Ever2DerogPubRec\"]==0].index\n",
    "print(\"Number of rows failing the test: \", test_13.shape[0])\n",
    "df = df.drop(test_13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All inconsistencies have now been dropped. Note that many of the retests passed. This indicated that inconsistent data was grouped together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Remap scales used for similar features\n",
    "    - Map *MaxDelqE2PublicREcLast12M* scale to match *MaxDelqEver* scale\n",
    "        -  Both features are measuring the same event (Delinquency) but are using different numerical mappings. One mapping is chose to be picked for consistency.\n",
    "        -  We will choose to use MaxDelqEver mapping as default as each number only appears once, simplifying the conversion\n",
    "    - Combine “unknown delinquency” & “current or never delinquent”\n",
    "        -  It is suspected that “unknown delinquency” is the equivalent of a “null value”. This should be checked with the domain expert. It is highly unlikely that a delinquency event would be omitted from the record. Therefore, in absence of any further information, I would recommend a “null value” is treated the same as “current or never delinquent” and should be mapped as such."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| MaxDelq2PublicRecLast12M | MaxDelqEver | meaning                      |\n",
    "|--------------------------|-------------|------------------------------|\n",
    "| 0                        | 2           | derogatory comment           |\n",
    "| 1                        | 3           | 120+ days delinquent         |\n",
    "| 2                        | 4           | 90 days delinquent           |\n",
    "| 3                        | 5           | 60 days delinquent           |\n",
    "| 4                        | 6           | 30 days delinquent           |\n",
    "| 5,6                      | 7           | unknown delinquency          |\n",
    "| 7                        | 8           | current and never delinquent |\n",
    "| 8,9                      | 9           | all other                    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 :Map *MaxDelqE2PublicREcLast12M* scale to match *MaxDelqEver* scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| MaxDelq2PublicRecLast12M | MaxDelqEver | meaning                      |\n",
    "|--------------------------|-------------|------------------------------|\n",
    "| 2                        | 2           | derogatory comment           |\n",
    "| 3                        | 3           | 120+ days delinquent         |\n",
    "| 4                        | 4           | 90 days delinquent           |\n",
    "| 5                        | 5           | 60 days delinquent           |\n",
    "| 6                        | 6           | 30 days delinquent           |\n",
    "| 7                        | 7           | unknown delinquency          |\n",
    "| 8                        | 8           | current and never delinquent |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Combine “unknown delinquency” & “current or never delinquent”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| MaxDelq2PublicRecLast12M | MaxDelqEver | meaning                      |\n",
    "|--------------------------|-------------|------------------------------|\n",
    "| 2                        | 2           | derogatory comment           |\n",
    "| 3                        | 3           | 120+ days delinquent         |\n",
    "| 4                        | 4           | 90 days delinquent           |\n",
    "| 5                        | 5           | 60 days delinquent           |\n",
    "| 6                        | 6           | 30 days delinquent           |\n",
    "| 7                        | 7           | current and never delinquent |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "# replacing all values to remporary value (12 = temp value 2, 13 = temp value 3 and so on....)\n",
    "df1 = df['MaxDelq2PublicRecLast12M']\n",
    "df1 = df1.replace(0, 12)\n",
    "df1 = df1.replace(1, 13)\n",
    "df1 = df1.replace(2, 14)\n",
    "df1 = df1.replace(3, 15)\n",
    "df1 = df1.replace(4, 16)\n",
    "df1 = df1.replace(5, 17)\n",
    "df1 = df1.replace(6, 17)\n",
    "df1 = df1.replace(7, 18)\n",
    "df1 = df1.replace(8, 19)\n",
    "df1 = df1.replace(9, 19)\n",
    "\n",
    "df1 = df1.replace(12, 2)\n",
    "df1 = df1.replace(13, 3)\n",
    "df1 = df1.replace(14, 4)\n",
    "df1 = df1.replace(15, 5)\n",
    "df1 = df1.replace(16, 6)\n",
    "df1 = df1.replace(17, 7)\n",
    "df1 = df1.replace(18, 8)\n",
    "df1 = df1.replace(19, 9)\n",
    "\n",
    "# step 2\n",
    "# shift values down for both features\n",
    "df1 = df1.replace(8, 7)\n",
    "df['MaxDelq2PublicRecLast12M'] = df1\n",
    "df['MaxDelq2PublicRecLast12M'] = df['MaxDelq2PublicRecLast12M'].astype('category')\n",
    "\n",
    "df1 = df['MaxDelqEver']\n",
    "df1 = df1.replace(8, 7)\n",
    "df['MaxDelqEver'] = df1\n",
    "df['MaxDelqEver'] = df['MaxDelqEver'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"MaxDelq2PublicRecLast12M\"].value_counts().plot(kind='bar', figsize=(12,10))\n",
    "print(\"Index value & Count\")\n",
    "print(df[\"MaxDelq2PublicRecLast12M\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"MaxDelqEver\"].value_counts().plot(kind='bar', figsize=(12,10))\n",
    "print(\"Index value & Count\")\n",
    "df[\"MaxDelqEver\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for MaxDelqEver values 8, -9 remain in the scale. The reason for this is unclear as the count is 0 for both. This will not effect analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Presence of outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Months since oldest trade open: Max is 598 months = 50 years. This is plausible \n",
    " - Months since most recent trade open: Max is 97 months = 8 years. This is plausible\n",
    " - Average months on file: Max is 240 months = 20 years. This is plausible\n",
    " - Number satisfactory trades: Max 78. This is plausible. Number of satisfactory trades increases in line with number of total trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='NumSatisfactoryTrades', axis=0, ascending=False, inplace=False, kind='quicksort', na_position='last')[['NumSatisfactoryTrades', 'NumTotalTrades']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Number of Derogatory trades 60, 90 late - Values seem plausible/logical and increase in line with each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='NumTrades60Ever2DerogPubRec', axis=0, ascending=False, inplace=False, kind='quicksort', na_position='last')[['NumTrades90Ever2DerogPubRec', 'NumTrades60Ever2DerogPubRec', 'NumTotalTrades']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Percentage trades never delinquent - low percentage values are a concern (see test 12). Need to speak to domain expert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='PercentTradesNeverDelq', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')[['PercentTradesNeverDelq', 'NumTrades60Ever2DerogPubRec', 'NumTotalTrades']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -  Months since most recent delinquency - genuine max is 83 months. This seems plausible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall are too many outliers to consider here. I could not deduce weather the outlier are wrong or not. Thus to be on the safe side I decided to keep them. The machine learning algorithms still need to deal with outliers in the real world as training data that is *too perfect and ideal* is just another form of bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of data quality plan:\n",
    "\n",
    "| Variable Names                     | Data Quality Issue            | Handling Strategy              |\n",
    "|------------------------------------|-------------------------------|--------------------------------|\n",
    "| ExternalRiskEstimate               | Outliers                      | Do Nothing                     |\n",
    "| MSinceOldestTradeOpen              | Negative Value -8 (29 rows)   | Replace with median            |\n",
    "| MSinceMostRecentTradeOpen          | Outliers                      | Do Nothing                     |\n",
    "| AverageMInFile                     | Outliers                      | Do Nothing                     |\n",
    "| NumSatisfactoryTrades              | Outliers                      | Do Nothing                     |\n",
    "| NumTrades60Ever2DerogPubRec        | Outliers                      | Do Nothing                     |\n",
    "| NumTrades90Ever2DerogPubRec        | Outliers                      | Do Nothing                     |\n",
    "| PercentTradesNeverDelq             | Outliers                      | Do Nothing                     |\n",
    "| MSinceMostRecentDelq               | Negative Value -8 (13 rows)   | Replace with median            |\n",
    "| MSinceMostRecentDelq               | Negative Value -7 ( 438 rows) | Replace with 125               |\n",
    "| MSinceMostRecentDelq               | Outliers                      | Do Nothing                     |\n",
    "| MaxDelq2PublicRecLast12M           | Scale                         | Replace with MaxDelqEver scale |\n",
    "| MaxDelq2PublicRecLast12M           | Scale                         | Combine index 7 & 8            |\n",
    "| MaxDelqEver                        | Scale                         | Combine index 7 & 8            |\n",
    "| NumTotalTrades                     | Outliers                      | Do Nothing                     |\n",
    "| NumTradesOpeninLast12M             | Outliers                      | Do Nothing                     |\n",
    "| PercentInstallTrades               | Outliers                      | Do Nothing                     |\n",
    "| MSinceMostRecentInqexcl7days       | Negative Value -8 (46 rows)   | Replace with median            |\n",
    "| MSinceMostRecentInqexcl7days       | Negative Value -7 (171 rows)  | Replace with 36                |\n",
    "| MSinceMostRecentInqexcl7days       | Outliers                      | Do Nothing                     |\n",
    "| NumInqLast6M                       | Outliers                      | Do Nothing                     |\n",
    "| NumInqLast6Mexcl7days              | Outliers                      | Do Nothing                     |\n",
    "| NetFractionRevolvingBurden         | Negative Value -8 (12 rows)   | Replace with median            |\n",
    "| NetFractionRevolvingBurden         | Outliers                      | Do Nothing                     |\n",
    "| NetFractionInstallBurden           | Negative Value -8 (319 rows)  | Replace with Null (*may need to revisit this later)              |\n",
    "| NetFractionInstallBurden           | Outliers                      | Do Nothing                     |\n",
    "| NumRevolvingTradesWBalance         | Negative Value -8 (10 rows)   | Replace with median            |\n",
    "| NumRevolvingTradesWBalance         | Outliers                      | Do Nothing                     |\n",
    "| NumInstallTradesWBalance           | Negative Value -8 (79 rows)   | Replace with median            |\n",
    "| NumInstallTradesWBalance           | Outliers                      | Do Nothing                     |\n",
    "| NumBank2NatlTradesWHighUtilization | Negative Value -8 (54 rows)   | Replace with median            |\n",
    "| NumBank2NatlTradesWHighUtilization | Outliers                      | Do Nothing                     |\n",
    "| PercentTradesWBalance              | Negative Value -8 (1 rows)    | Replace with median            |\n",
    "| PercentTradesWBalance              | Outliers                      | Do Nothing                     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Save the new CSV file with a self explanatory name. Save the data quality plan to a single PDF file (as a table or a structured text)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data quality plan saved as a picture and available in Data_Quality_Plan.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print table with continuous statistics\n",
    "continuous_columns = df.select_dtypes(['int64']).columns\n",
    "df[continuous_columns].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print table with categorical statistics\n",
    "df.select_dtypes(['category']).describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the cleaned dataframe to a csv file\n",
    "df.to_csv('CreditRisk_1-2_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3). Exploring relationships between feature pairs:\n",
    "\n",
    "### - Choose a subset of features you find promising and plot pairwise feature interactions (e.g., continuous-continuous feature plot or continuous-categorical plots or correlation plots). Explain your choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features that I will look at for pairwise feature interaction are the following continuous features:\n",
    "- External Risk Estimate: \n",
    "- Percent of trades never delinquent: \n",
    "- Number of trades 60 days late:\n",
    "- Months since most recent delinquency\n",
    "- Number of trades open in the last 12 months\n",
    "- Percent install trades\n",
    "- Number of enquiries in last 6 months excluding 7 days\n",
    "- Number Revolving trades with balance\n",
    "- Number install trades with balance\n",
    "\n",
    "and the following categorical features:\n",
    "- Risk performance\n",
    "- Max delinquency in last 12 month\n",
    "- Max delinquency ever\n",
    "\n",
    "The choices here I based on the idea that these particular features would be strong markers to indicate the credit risk of an applicant, as also discussed based on plots in the Data_Quality_report.pdf.\n",
    "\n",
    "### Correlations for the numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix using code found on https://stanford.edu/~mwaskom/software/seaborn/examples/many_pairwise_correlations.html\n",
    "sns.set(style=\"white\")\n",
    "continuous_columns1 = ['ExternalRiskEstimate',\n",
    "                      'PercentTradesNeverDelq',\n",
    "                      'NumTrades60Ever2DerogPubRec',\n",
    "                      'MSinceMostRecentDelq',\n",
    "                      'NumTradesOpeninLast12M' ,\n",
    "                      'PercentInstallTrades',\n",
    "                      'NumInqLast6Mexcl7days',\n",
    "                      'NetFractionRevolvingBurden',\n",
    "                      'NumInstallTradesWBalance',\n",
    "                      'NumBank2NatlTradesWHighUtilization']\n",
    "# Calculate correlation of all pairs of continuous features\n",
    "corr = df[continuous_columns1].corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom colormap - blue and red\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, annot=True, mask=mask, cmap=cmap, vmax=1, vmin=-1,\n",
    "            square=True, xticklabels=True, yticklabels=True,\n",
    "            linewidths=.5, cbar_kws={\"shrink\": .5}, ax=ax)\n",
    "plt.yticks(rotation = 0)\n",
    "plt.xticks(rotation = 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='scatter', x='PercentTradesNeverDelq', y='ExternalRiskEstimate')\n",
    "df.plot(kind='scatter', x='MSinceMostRecentDelq', y='ExternalRiskEstimate')\n",
    "df.plot(kind='scatter', x='MSinceMostRecentDelq', y='PercentTradesNeverDelq')\n",
    "df.plot(kind='scatter', x='NetFractionRevolvingBurden', y='ExternalRiskEstimate')\n",
    "df.plot(kind='scatter', x='NumRevolvingTradesWBalance', y='NumBank2NatlTradesWHighUtilization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The continuous features above were chosen as we believed they could have a strong impact on risk outcome.\n",
    "- We can see from the above correlation matrix that there are a couple of correlations worth discussing (We have omitted some features that would have very high correlation but provide no additional information e.g. enquiries in last 6 months vs enquiries  in last 6 months excl last 7 days).\n",
    "- We can see the impact that the -8 special value is having for percentage trades never delinquent (transformed into 125) but even excluding it their is no strong correlation.\n",
    "- We see a correlation (0.53) between the external risk estimate and the percentage of trades never delinquent.\n",
    "- We see a correlation (-0.63) between the external risk estimate and the net fraction of trades with burden \n",
    "- From this we do not get much additional information only that the external risk estimate looks like a good marker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical vs Categorical feature plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using code from the module lab\n",
    "maxDelqEver = pd.unique(df[\"MaxDelqEver\"].ravel())\n",
    "\n",
    "# add new column and set values to zero\n",
    "df['percent'] = 0\n",
    "\n",
    "#print header\n",
    "print(\"MaxDelqEver\")\n",
    "print(\"Index \\t Count\")\n",
    "\n",
    "# for each delinquency category\n",
    "for i in maxDelqEver:\n",
    "    \n",
    "    count = df[df[\"MaxDelqEver\"] == i].count()['RiskPerformance']\n",
    "    count_percentage = (1 / count) * 100\n",
    "        \n",
    "    # print out index vs count\n",
    "    print(i, \"\\t\", count)\n",
    "    \n",
    "    index_list = df[df['MaxDelqEver'] == i].index.tolist()\n",
    "    for ind in index_list:\n",
    "        df.loc[ind, 'percent'] = count_percentage\n",
    "        \n",
    "group = df[['percent','MaxDelqEver','RiskPerformance']].groupby(['MaxDelqEver','RiskPerformance']).sum()\n",
    "\n",
    "my_plot = group.unstack().plot(kind='bar', stacked=True, title=\"Risk vs MaxDelqEver\", figsize=(15,7), grid=True)\n",
    "\n",
    "# add legend\n",
    "#red_patch = mpatches.Patch(color='orange', label='Good')\n",
    "#blue_patch = mpatches.Patch(color='blue', label='Bad')\n",
    "#my_plot.legend(handles=[red_patch, blue_patch], frameon = True)\n",
    "my_plot.legend([\"Good Outcome\", \"Bad Outcome\"])\n",
    "    \n",
    "# add gridlines\n",
    "plt.grid(b=True, which='major', color='#666666', linestyle='-')\n",
    "plt.minorticks_on()\n",
    "plt.grid(b=True, which='minor', color='#999999', linestyle='-', alpha=0.2)\n",
    "\n",
    "my_plot.set_xlabel(\"Delinquency index\")\n",
    "my_plot.set_ylabel(\"% Risk\")\n",
    "my_plot.set_ylim([0,100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the stacked bar plot for Delinquency against the target risk outcome. \n",
    "- On first indication the figures are not as expected.\n",
    "- There is no clear trend of decreasing likelihood of \"bad\" outcomes as delinquency scale improves.\n",
    "- Only when we reach index 7, which means no delinquency do we see a significant drop.\n",
    "- This indicates that a financial institution will look at any type of delinquency, either derogatory, 30, 60 or 90 days late in the same light.\n",
    "- Judging from this we could derive a new feature of with a binary value, delinquent or not, without losing much information.\n",
    "- In summary \n",
    "    -  If entry has ever had a delinquency there is approx 60% likelihood of a bad risk marker\n",
    "    -  If entry has never had delinquency there is approx 40% likelihood of a bad risk marker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using code from the module lab\n",
    "maxDelqEver = pd.unique(df[\"MaxDelq2PublicRecLast12M\"].ravel())\n",
    "\n",
    "# add new column and set values to zero\n",
    "df['percent'] = 0\n",
    "\n",
    "#print header\n",
    "print(\"MaxDelq2PublicRecLast12M\")\n",
    "print(\"Index \\t Count\")\n",
    "\n",
    "# for each income level\n",
    "for i in maxDelqEver:\n",
    "    \n",
    "    count = df[df[\"MaxDelq2PublicRecLast12M\"] == i].count()['RiskPerformance']\n",
    "    count_percentage = (1 / count) * 100\n",
    "    \n",
    "    # print out index vs count\n",
    "    print(i, \"\\t\", count)\n",
    "    \n",
    "    index_list = df[df['MaxDelq2PublicRecLast12M'] == i].index.tolist()\n",
    "    for ind in index_list:\n",
    "        df.loc[ind, 'percent'] = count_percentage\n",
    "        \n",
    "group = df[['percent','MaxDelq2PublicRecLast12M','RiskPerformance']].groupby(['MaxDelq2PublicRecLast12M','RiskPerformance']).sum()\n",
    "\n",
    "my_plot = group.unstack().plot(kind='bar', stacked=True, title=\"Risk vs MaxDelq2PublicRecLast12M\", figsize=(15,7))\n",
    "\n",
    "# add legend\n",
    "#red_patch = mpatches.Patch(color='orange', label='Good')\n",
    "#blue_patch = mpatches.Patch(color='blue', label='Bad')\n",
    "#my_plot.legend(handles=[red_patch, blue_patch], frameon = True)\n",
    "my_plot.legend([\"Good Outcome\", \"Bad Outcome\"])\n",
    "\n",
    "# add gridlines\n",
    "plt.grid(b=True, which='major', color='#666666', linestyle='-')\n",
    "plt.minorticks_on()\n",
    "plt.grid(b=True, which='minor', color='#999999', linestyle='-', alpha=0.2)\n",
    "\n",
    "# set labels\n",
    "my_plot.set_xlabel(\"Delinquency index\")\n",
    "my_plot.set_ylabel(\"% Risk\")\n",
    "my_plot.set_ylim([0,100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the stacked bar plot for Delinquency in the last 12 months against the target risk outcome. \n",
    "\n",
    "- Similar to the \"max delinquency ever\" scale above, on first indication the figures are not as expected.\n",
    "- There is no clear trend of decreasing number of \"bad\" outcomes as delinquency  scale improves.\n",
    "- For some of the index vales there are very few entries. For example index 4 has count of 4, index 3 has a count of 2. Index 6 and 7 dominate in terms of count.\n",
    "- Again this reinforces the view that a financial institution will look at any type of delinquency, either derogatory, 30, 60 or 90 days late in the same light.\n",
    "- Similarly we could derive a new feature with a binary value, delinquent or not in last 12 months, without losing much information.\n",
    "- In summary \n",
    "    - If entry has had a delinquency in last 12 months there is approx 75% likelihood of a bad risk marker\n",
    "        - This indicates a more recent delinquency has a stronger impact. (75% vs 60%) This is as expected.\n",
    "    - If entry has not had delinquency in last 12 months there is approx 50% likelihood of a bad risk marker\n",
    "        - This indicates the narrower time scale for this measurement (12 months vs ever) has a bearing on higher risk (50% vs 40%). This is as expected.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous-categorical feature plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Plot Months since most recent dleinquency vs Risk marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "flierprops = dict(marker='o', markerfacecolor='green', markersize=6,\n",
    "                  linestyle='none')\n",
    "df.boxplot(column=['MSinceMostRecentDelq'], by=['RiskPerformance'], flierprops=flierprops, figsize=(10,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['RiskPerformance'] == \"Good\"][['MSinceMostRecentDelq']].plot(kind='hist',figsize=(7,7), bins=10, title=\"Good risk marker\")\n",
    "plt.grid(b=True, which='major', color='#666666', linestyle='-')\n",
    "plt.minorticks_on()\n",
    "plt.grid(b=True, which='minor', color='#999999', linestyle='-', alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['RiskPerformance'] == \"Bad\"][['MSinceMostRecentDelq']].plot(kind='hist',figsize=(7,7), bins=10, title=\"Bad risk marker\")\n",
    "plt.grid(b=True, which='major', color='#666666', linestyle='-')\n",
    "plt.minorticks_on()\n",
    "plt.grid(b=True, which='minor', color='#999999', linestyle='-', alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - We can see from the above histograms that there is significantly more people with a delinqency who have a bad outcome as expected\n",
    "  - We can see from the above box plots that the median value for a bad outcome is approximately 30 months while the median value for a good outcome is 125 months (-8 special value was changed to 125 months). This is expected \n",
    "  - We can conclude that delinquency is a good risk marker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Plot Percentage of Trades With Balance vs Risk marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "flierprops = dict(marker='o', markerfacecolor='green', markersize=6,\n",
    "                  linestyle='none')\n",
    "df.boxplot(column=['PercentTradesWBalance'], by=['RiskPerformance'], flierprops=flierprops, figsize=(10,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['RiskPerformance'] == \"Good\"][['PercentTradesWBalance']].plot(kind='hist',figsize=(7,7), bins=10, title=\"Good risk marker\")\n",
    "plt.grid(b=True, which='major', color='#666666', linestyle='-')\n",
    "plt.minorticks_on()\n",
    "plt.grid(b=True, which='minor', color='#999999', linestyle='-', alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['RiskPerformance'] == \"Bad\"][['PercentTradesWBalance']].plot(kind='hist',figsize=(7,7), bins=10, title=\"Bad risk marker\")\n",
    "plt.grid(b=True, which='major', color='#666666', linestyle='-')\n",
    "plt.minorticks_on()\n",
    "plt.grid(b=True, which='minor', color='#999999', linestyle='-', alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - We can see here that a higher % trades with a balance results in a higher likliehood of bad risk outcome\n",
    "   - From the histograms above we see that there is a much lower likliehood of a good risk outcome if % trade with a balance is in the 80-100% range\n",
    "   - The interquartile range of the bad outcom (median=75%) is higher than the good outcome (median=60%)  reinforcing the observations from the histograms\n",
    "   - We can conclude that % utilisation is a good risk marker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Plot External Risk Estimate vs Risk marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "flierprops = dict(marker='o', markerfacecolor='green', markersize=6,\n",
    "                  linestyle='none')\n",
    "df.boxplot(column=['ExternalRiskEstimate'], by=['RiskPerformance'], flierprops=flierprops, figsize=(10,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['RiskPerformance'] == \"Good\"][['ExternalRiskEstimate']].plot(kind='hist',figsize=(7,7), bins=10, title=\"Bad risk marker\")\n",
    "plt.grid(b=True, which='major', color='#666666', linestyle='-')\n",
    "plt.minorticks_on()\n",
    "plt.grid(b=True, which='minor', color='#999999', linestyle='-', alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['RiskPerformance'] == \"Bad\"][['ExternalRiskEstimate']].plot(kind='hist',figsize=(7,7), bins=10, title=\"Bad risk marker\")\n",
    "plt.grid(b=True, which='major', color='#666666', linestyle='-')\n",
    "plt.minorticks_on()\n",
    "plt.grid(b=True, which='minor', color='#999999', linestyle='-', alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -  We can see from the above box plots that the median external risk estimate for a good outcome is about 76 and for a bad outcome it is 66. The difference is maybe not a big as expected. A larger number is more positive\n",
    " -  This can also be seen from the above histograms. In the upper range of the external risk estimate (80+) there are very few with a bad outcome.\n",
    " -  There is a clear trend here so we can say the external risk estimate does have an impact on the overall risk outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Plot Total Number trade vs Risk marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "flierprops = dict(marker='o', markerfacecolor='green', markersize=6,\n",
    "                  linestyle='none')\n",
    "df.boxplot(column=['NumTotalTrades'], by=['RiskPerformance'], flierprops=flierprops, figsize=(10,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['RiskPerformance'] == \"Good\"][['NumTotalTrades']].plot(kind='hist',figsize=(7,7), bins=10, title=\"Good risk marker\")\n",
    "plt.grid(b=True, which='major', color='#666666', linestyle='-')\n",
    "plt.minorticks_on()\n",
    "plt.grid(b=True, which='minor', color='#999999', linestyle='-', alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['RiskPerformance'] == \"Bad\"][['NumTotalTrades']].plot(kind='hist',figsize=(7,7), bins=10, title=\"Bad risk marker\")\n",
    "plt.grid(b=True, which='major', color='#666666', linestyle='-')\n",
    "plt.minorticks_on()\n",
    "plt.grid(b=True, which='minor', color='#999999', linestyle='-', alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -  We can see from the above box plots that the median number of accounts opened with a good outcome is 23 and for a bad outcome it is 20. The difference is minimal\n",
    " -  This can also be seen from the above histograms. There is no clear difference between both\n",
    " -  There is no clear trend here so we can say the number of trades does not have a big effect on the risk outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Discuss your findings from the plots above. Do you find any features or feature combinations that are indicative of the target outcome (i.e., churn)? Explain in plain words (a short paragraph) the story of your findings so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Continuous vs Continuous\n",
    "    - We picked a number of continuous features that we would expect to have a stronger impact on risk outcome however we do not see many features with a strong connection to each other.\n",
    "    - We see a connection between the external risk estimate and the net fraction of trades with burden and also the percentage of trades never delinquent. \n",
    "    -  This does not give us much additional information only that the external risk estimate looks like a good marker\n",
    "- Categorical vs Categorical\n",
    "    - We see no strong connection between the type of delinquency and the likelihood of a bad risk outcome for both delinquencies in the last 12 months and in the total lifetime. \n",
    "    - If the delinquency is 30 days late or 90 days late they appear to be treated the same. \n",
    "    - We do see a significant drop in the likelihood of a good risk outcome if the entry has been delinquent. This is as expected and makes a strong case for having a binary feature, marking if a entry has been delinquent or not.\n",
    "- Continuous vs Categorical\n",
    "    - There is a strong connection between % utilisation, months since most recent delinquency and external risk estimate.\n",
    "    - The higher the % utilisation, a more recent delinquency and a lower external risk estimate are the likely predictors of a bad risk outcome.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (4). Transform, extend or combine the existing features to create a few new features (at least 3)\n",
    "\n",
    "### Aim to better capture the problem domain and the target outcome. Justify the steps and choices you are making. Add these features to your clean dataset and save it as a CSV file with a self explanatory name. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **New feature 1** \n",
    "- DelqEver : Measures if a entry during their history has ever been delinquent\n",
    "- From analysis of the data if an entry has been delinquent, the type of delinquency is not so important\n",
    "- Therefore it is makes sense to derive a feature with a binary outcome, delinquent or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['DelqEver'] = df['MaxDelq2PublicRecLast12M']!=7\n",
    "df['DelqEver'] = df['MaxDelqEver']!=7\n",
    "df['DelqEver'] = df['DelqEver'].astype('category')\n",
    "df[\"DelqEver\"].value_counts().plot(kind='bar', figsize=(12,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate stacked bar plot DelqEver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using code from the module lab\n",
    "maxDelqEver = pd.unique(df['DelqEver'].ravel())\n",
    "\n",
    "# add new column and set values to zero\n",
    "df['percent'] = 0\n",
    "\n",
    "#print header\n",
    "print('DelqEver')\n",
    "print(\"Index \\t Count\")\n",
    "\n",
    "# for each income level\n",
    "for i in maxDelqEver:\n",
    "    \n",
    "    count = df[df['DelqEver'] == i].count()['RiskPerformance']\n",
    "    count_percentage = (1 / count) * 100\n",
    "    \n",
    "    # print out index vs count\n",
    "    print(i, \"\\t\", count)\n",
    "    \n",
    "    index_list = df[df['DelqEver'] == i].index.tolist()\n",
    "    for ind in index_list:\n",
    "        df.loc[ind, 'percent'] = count_percentage\n",
    "        \n",
    "group = df[['percent','DelqEver','RiskPerformance']].groupby(['DelqEver','RiskPerformance']).sum()\n",
    "\n",
    "my_plot = group.unstack().plot(kind='bar', stacked=True, title=\"Risk vs DelqEver\", figsize=(15,7))\n",
    "\n",
    "# add legend\n",
    "red_patch = mpatches.Patch(color='orange', label='Good')\n",
    "blue_patch = mpatches.Patch(color='blue', label='Bad')\n",
    "my_plot.legend(handles=[red_patch, blue_patch], frameon = True)\n",
    "\n",
    "# add gridlines\n",
    "plt.grid(b=True, which='major', color='#666666', linestyle='-')\n",
    "plt.minorticks_on()\n",
    "plt.grid(b=True, which='minor', color='#999999', linestyle='-', alpha=0.2)\n",
    "\n",
    "# set labels\n",
    "my_plot.set_xlabel(\"Delinquency index\")\n",
    "my_plot.set_ylabel(\"% Risk\")\n",
    "my_plot.set_ylim([0,100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **New feature 2** \n",
    "- DelqLast12M : Measures if a entry during the last 12 months has ever been delinquent\n",
    "- From analysis of the data if an entry has been delinquent, the type of delinquency is not so important\n",
    "- Therefore it is makes sense to derive a feature with a binary outcome, delinquent or not in last 12 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DelqLast12M'] = df['MaxDelq2PublicRecLast12M']!=7\n",
    "df['DelqLast12M'] = df['DelqLast12M'].astype('category')\n",
    "df[\"DelqLast12M\"].value_counts().plot(kind='bar', figsize=(12,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate stacked bar plot DelqLast12M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using code from the module lab\n",
    "maxDelqEver = pd.unique(df[\"DelqLast12M\"].ravel())\n",
    "\n",
    "# add new column and set values to zero\n",
    "df['percent'] = 0\n",
    "\n",
    "#print header\n",
    "print(\"DelqLast12M\")\n",
    "print(\"Index \\t Count\")\n",
    "\n",
    "# for each income level\n",
    "for i in maxDelqEver:\n",
    "    \n",
    "    count = df[df[\"DelqLast12M\"] == i].count()['RiskPerformance']\n",
    "    count_percentage = (1 / count) * 100\n",
    "    \n",
    "    # print out index vs count\n",
    "    print(i, \"\\t\", count)\n",
    "    \n",
    "    index_list = df[df[\"DelqLast12M\"] == i].index.tolist()\n",
    "    for ind in index_list:\n",
    "        df.loc[ind, 'percent'] = count_percentage\n",
    "        \n",
    "group = df[['percent',\"DelqLast12M\",'RiskPerformance']].groupby([\"DelqLast12M\",'RiskPerformance']).sum()\n",
    "\n",
    "my_plot = group.unstack().plot(kind='bar', stacked=True, title=\"Risk vs DelqLast12M\", figsize=(15,7))\n",
    "\n",
    "# add legend\n",
    "red_patch = mpatches.Patch(color='orange', label='Good')\n",
    "blue_patch = mpatches.Patch(color='blue', label='Bad')\n",
    "my_plot.legend(handles=[red_patch, blue_patch], frameon = True)\n",
    "\n",
    "# add gridlines\n",
    "plt.grid(b=True, which='major', color='#666666', linestyle='-')\n",
    "plt.minorticks_on()\n",
    "plt.grid(b=True, which='minor', color='#999999', linestyle='-', alpha=0.2)\n",
    "\n",
    "# set labels\n",
    "my_plot.set_xlabel(\"Delinquency index\")\n",
    "my_plot.set_ylabel(\"% Risk\")\n",
    "my_plot.set_ylim([0,100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **New Feature 3**\n",
    "- PercentSatisfactoryTrades: Measure the Percentage of satisfactory trades\n",
    "- Will allow a comparison between the definition of a satisfactory trade vs a trade never delinquent\n",
    "- Would expect them to be equivalent but they are not\n",
    "- This derived feature brings up number of questions that will need to be address by the domain expert. We need to find the reason for the difference between the two. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PercentSatisfactoryTrades'] = round(((df['NumSatisfactoryTrades'] / df['NumTotalTrades']) *100),0)\n",
    "df[['PercentSatisfactoryTrades','NumSatisfactoryTrades','NumTotalTrades']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['PercentSatisfactoryTrades','PercentTradesNeverDelq']].head()\n",
    "# find the number of entries where % satisfactory trades == % trades never delinquent\n",
    "print(\"Number of trades where %satisfactory == % never delinquent:\",df[df['PercentSatisfactoryTrades']==df['PercentTradesNeverDelq']].shape[0])\n",
    "print(\"Number of trades where %satisfactory < % never delinquent:\",df[df['PercentSatisfactoryTrades']<df['PercentTradesNeverDelq']].shape[0])\n",
    "print(\"Number of trades where %satisfactory > % never delinquent:\",df[df['PercentSatisfactoryTrades']>df['PercentTradesNeverDelq']].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **New Feature 4**\n",
    "- NumTradesWBalance: Number of trades with balance = (percentage of trades with a balance * number  total trades) / 100\n",
    "- This derived feature should return an integer but it is likely the original % figure has been rounded.\n",
    "- Will use floor division to be conservative (ensure int values returned) to account for this\n",
    "- We can see that num revolving trades + num install trades + num bank trades != num total trades\n",
    "    -  Therefore we need to estimate number trades with balance using percentage of trades with balance and total trades as it will give a more accurate value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NumTradesWBalance'] = (df['PercentTradesWBalance'] * df['NumTotalTrades'])//100\n",
    "df[['NumTradesWBalance', 'PercentTradesWBalance', 'NumTotalTrades']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['NumRevolvingTradesWBalance','NumInstallTradesWBalance', 'NumBank2NatlTradesWHighUtilization','NumTotalTrades']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check logical integrity of derived data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Check if any entries have a delinquency in last 12 months and no delinquency ever (impossible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_11 = df[['DelqLast12M','DelqEver']][df['DelqLast12M']==True][df['DelqEver']==False]\n",
    "print(\"Number of rows failing the test: \", test_11.shape[0])\n",
    "test_11.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(test_11.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- checking if any entries have a percent trades never delinquent < 100% and no delinquency ever (impossible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_12 = df[['PercentTradesNeverDelq','DelqEver']][df['PercentTradesNeverDelq']<100][df['DelqEver']==False]\n",
    "print(\"Number of rows failing the test: \", test_12.shape[0])\n",
    "test_12.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(test_12.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- checking if any entries have a percent trades delinquent <100% and no delinquency ever (impossible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_13 = df[['MSinceMostRecentDelq','DelqEver']][df['MSinceMostRecentDelq']<125][df['DelqEver']==False]\n",
    "print(\"Number of rows failing the test: \", test_13.shape[0])\n",
    "test_13.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To finish - Print data types, descriptive tables, save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print continuous statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print table with continuous statistics\n",
    "continuous_columns = df.select_dtypes(['int64','float64']).columns\n",
    "df[continuous_columns].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print table with categorical statistics\n",
    "df.select_dtypes(['category']).describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the cleaned dataframe to a csv file\n",
    "df.to_csv('CreditRisk_1-3_cleaned_new_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
